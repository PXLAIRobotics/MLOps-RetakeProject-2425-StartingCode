#+TITLE: MLOps/Big Data Dataset Preparation & Infrastructure Template
#+AUTHOR: Tim Dupont
#+DATE: 2025
#+OPTIONS: toc:t num:nil

* Overview

This repository provides a reusable and modular foundation for MLOps and Big Data projects.

It is designed to accelerate dataset acquisition and experimentation, and to provide a starting point for MLOps pipeline development and Big Data infrastructure setups.

It includes:

- A containerised (Docker-based) Ubuntu 24.04 LTS "devbox"
  - Equipped with essential tools (e.g., Python, VisiData, ...) and easily extensible via `pip3 install` & `requirements.txt`, and a clean Dockerfile.
  - Ready to download and split necessary data set(s).
    - Includes scripts to retrieve datasets from Kaggle and store them in a persistent volume
    - Out-of-the-box access to configuration files of third-party credentials (e.g., `kaggle.json` in ./config/kaggle)
    - Provides utilities for chunking or splitting time-series data (e.g., weekly splits like for Favorita)
- Clearly defined folders for persistent data, configuration, and scripts



* ğŸš€ Setup & Workflow Guide

Follow these steps to get started, download example data (e.g., the Favorita dataset), and begin your project workflow.

** 1. **Clone this repository**

   Run the following commands in a Unix-like environment (Linux, macOS, or WSL).
   Note: Native Windows terminals are not recommended due to compatibility issues.

   #+BEGIN_SRC bash
     git clone git@github.com:PXLAIRobotics/MLOps-RetakeProject-2425-StartingCode.git
     # Or: gh repo clone PXLAIRobotics/MLOps-RetakeProject-2425-StartingCode
     # Or: git clone https://github.com/PXLAIRobotics/MLOps-RetakeProject-2425-StartingCode.git
     # Or: go to https://github.com/PXLAIRobotics/MLOps-RetakeProject-2425-StartingCode and download the ZIP and extract it.
     cd MLOps-RetakeProject-2425-StartingCode
   #+END_SRC

** 2. **Get familiar with the structure**

   Read the structure overview depicted below to understand the purpose of:
   - `commands/`
   - `config/`
   - `scripts/`
   - `infrastructure/devbox/`
   - `data/` and its subdirectory `raw_data/`
   - `config/kaggle/`

  ğŸ—‚ï¸ Repository Structure

  #+BEGIN_SRC bash
    .
    â”œâ”€â”€ commands/                         # CLI scripts and wrappers
    â”œâ”€â”€ config/                           # Secrets/configs (excluded from version control)
    â”‚   â””â”€â”€ kaggle/                       #    Place to put your local kaggle.json
    â”œâ”€â”€ data/                             # Local data
    â”‚   â””â”€â”€ raw_data/                     #    Raw data output (after download/extraction/splitting)
    â”œâ”€â”€ documentation/                    # A place to put documentation
    â”œâ”€â”€ infrastructure/                   # Put all infrastructure code here.
    â”‚   â””â”€â”€ devbox/                       #    # Self-contained Docker-based devbox environment
    â”œâ”€â”€ scripts/                          # Python utilities for data handling
    â””â”€â”€ README.org                        # This readme.
  #+END_SRC


** 3. **Build the devbox**

   This sets up a containerised Python environment with all dependencies:

   #+BEGIN_SRC bash
     cd infrastructure/devbox
     ./001_build_image.bash
   #+END_SRC

** 4. **Start the devbox**
   
   #+BEGIN_SRC bash
     ./002_start_devbox.bash
   #+END_SRC

   This mounts your folders into the container:
   | DIR on repository | DIR in container | Extra information                             |
   |-------------------+------------------+-----------------------------------------------|
   | `â”œâ”€â”€ commands/`   | `/commands       | This directory is automatically added to PATH |
   | `â”œâ”€â”€ config/`     | `/config`        |                                               |
   | `â”œâ”€â”€ data/`       | `/data`          |                                               |
   | `â”œâ”€â”€ scripts/`    | `/scripts`       | This directory is automatically added to PATH |
   |-------------------+------------------+-----------------------------------------------|

   This `.bash` script can be easily extended with extra volumes if needed.

** 5. **Get your Kaggle API key**

   - Go to: https://www.kaggle.com/account
   - Click â€œCreate New API Tokenâ€ under the *API* section
   - This downloads a file: `kaggle.json`

** 6. **Place the API key into the correct folder**

   Put the `kaggle.json` file in the `config/kaggle` directory on your host machine, it will automatically be available in the container.

** 7. **Accept the Kaggle dataset terms** (if needed!)

   In this example we are going to use the Favorita Grocery Sales Forecasting dataset. Therefore, we need to accept the terms of this dataset.

   Visit the dataset page and click "Join Competition", and follow the necessary steps.
   https://www.kaggle.com/competitions/favorita-grocery-sales-forecasting

** 8. **Download the Favorita dataset**

   Inside the devbox:

   #+BEGIN_SRC bash
     run_kaggle_download_script /scripts/download_favorita.py
   #+END_SRC

   This will download the dataset (if `kaggle.json` is configured and the terms are accepted) and extract it into `/data`.

** 9. **Explore the data**

   The data will be located in:

   #+BEGIN_SRC bash
     data/raw_data/favorita-grocery-sales-forecasting/
   #+END_SRC

   You can explore the data using:
   - Your own Python scripts (place them in `/scripts`)
   - Or the excellent terminal-based tool `visidata` ([[https://www.visidata.org/][VisiData: Open-source data multitool]])

     For example:
     #+BEGIN_SRC bash
       vd /data/raw_data/favorita-grocery-sales-forecasting/train.csv
     #+END_SRC

     Inspect all files.

     *Pro tip*: Keep an exploration log in Markdown to stay organized and avoid information overload.

** 10. **Read the project assignment**

    Consult the retake project assignment brief of the MLOps and/or Big Data course.

** 11. **Check out the weekly train split script for Favorita**

    #+BEGIN_SRC bash
      $ /scripts/split_favorita_train_in_weeks.py 
      â— No valid option provided. Use one of:
         --overview                         Show dataset summary
         --all                              Split full dataset by week
         --from DATE --to DATE              Split only specific date range
         --year YYYY --weeks N              Split N weeks from ISO Week 1
         --year YYYY --start-week W --weeks N  Start from ISO Week W
    #+END_SRC

The `train.csv` file is quite large, so splitting it into smaller weekly files may improve performance and enable meaningful MLOps or Big Data operations.

    #+BEGIN_SRC bash
      $ /scripts/split_favorita_train_in_weeks.py --overview
      Scanning dataset for date overview...

      ğŸ“Š Dataset Overview:
      - Oldest date : 2013-01-01
      - Newest date : 2017-08-15
      - Total days  : 1688
      - Total weeks : 241
      - Total years : 4.62
      
    #+END_SRC

    This tool allows you to split the `train.csv` file into weekly chunks.

** 12. **Split the Favorita data as needed**

    Examples:

    - Split the entire dataset (This will take a lot of time.)
      
      #+BEGIN_SRC bash
        $ /scripts/split_favorita_train_in_weeks.py --all
        ...
      #+END_SRC

            The output is too verbose to include in this guide.

    - Split a specific year and number of weeks:
      #+BEGIN_SRC bash
        $ /scripts/split_favorita_train_in_weeks.py --year 2016 --start-week 10 --weeks 5
        ğŸ—“ï¸  Splitting 5 week(s) starting from Week 10, 2016
        From 2016-03-07 to 2016-04-10
        ğŸ“¦ Splitting data from 2016-03-07 to 2016-04-10
        /scripts/split_favorita_train_in_weeks.py:49: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.
          for chunk in pd.read_csv(INPUT_FILE, parse_dates=["date"], chunksize=CHUNK_SIZE):
        ğŸ“ Writing weekly files to: /data/raw_data/favorita-grocery-sales-forecasting/weeks
        âœ… Saved /data/raw_data/favorita-grocery-sales-forecasting/weeks/train_2016-W10.csv â€” 662413 rows
        âœ… Saved /data/raw_data/favorita-grocery-sales-forecasting/weeks/train_2016-W11.csv â€” 665398 rows
        âœ… Saved /data/raw_data/favorita-grocery-sales-forecasting/weeks/train_2016-W12.csv â€” 657875 rows
        âœ… Saved /data/raw_data/favorita-grocery-sales-forecasting/weeks/train_2016-W13.csv â€” 681864 rows
        âœ… Saved /data/raw_data/favorita-grocery-sales-forecasting/weeks/train_2016-W14.csv â€” 674518 rows
      #+END_SRC

** 13. **Do your project work**

    Use the weekly datasets, train models, explore drift, build pipelines â€” whatever your assignment requires.

** 14. **Iterate**

    As your project evolves, keep refining your work by:
    - Revisit step 10 regularly to stay aligned with the project requirements.
    - Repeat step 12 (with new split configs)
    - Revisit steps 9â€“11 to explore new slices of data or experiments
    - Continue step 13 until your project(s) is(are) completed



* ğŸ“ infrastructure/

Use this directory to implement the requested architecture using Docker compose and all related and necessary tools.
Use the devbox as inspiration. Leverage Docker volumes for persistent storage and shared data access between containers if needed.
You can also add sub-directories in `commands, config, scripts, ...` and use those as volumes in order to segregate scripts for specific containers.


* ğŸ“ scripts/

Add additional scripts to this directory. Itâ€™s recommended to organize them into subdirectories.
You may also create top-level folders like `src/` if your project requires it.

* ğŸ“ documentation/

Put all documentation in this directory.

* ğŸ“Œ License / Contribution

Feel free to fork, modify, or reuse this layout. Contributions or suggestions are welcome.
